{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import string\n",
    "import operator\n",
    "import math\n",
    "import string\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.cluster import Birch\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pikepdf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_clusters(X, labels):\n",
    "    \"\"\"\n",
    "    Takes in the datapoints and the cluster labels and then construcs a scatterplot indicatiing  each custer \n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    plt.scatter(X[:,1], X[:,0], c=labels)\n",
    "    plt.show()\n",
    "    \n",
    "def viz_clusters_after(X, labels):\n",
    "    labels =list(set(X))\n",
    "    plt.plot(X[:,1], X[:,0], c=labels)\n",
    "    \n",
    "def draw_table(norm_X,mean,closest):\n",
    "    count = 0\n",
    "    sentences = []\n",
    "    for i in range(len(mean)):\n",
    "        sentences_with_label = [sent[1] for sent in norm_X if sent[0] == mean[i][0]]\n",
    "        for sent in sentences_with_label:\n",
    "            sentence_row = (count,sent,mean[i][0],mean[i][1],closest[i][1])\n",
    "            sentences.append(sentence_row)\n",
    "            count +=1\n",
    "    return np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_threshold(sentences):\n",
    "    return int(0.5 * len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0  SENTENCE PREPROCESSING\n",
    "def preprocess(words):\n",
    "    stp = stopwords.words('english')\n",
    "    white_list = ['Allah', 'God']\n",
    "    words = [word for word in words if word not in stp]\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    words = [word.lower() for word in words]\n",
    "    return words\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmed_words = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        wrd = wnl.lemmatize(word)\n",
    "        lemmed_words.append(wrd)\n",
    "    return lemmed_words\n",
    "\n",
    "def tagger(sentence):\n",
    "    \"\"\"\n",
    "    takes a whole sentence as input and gets the Part-Of-Speech tag for each word in the sentence\n",
    "    \"\"\"\n",
    "    acceptable_words = []\n",
    "    word_tags = pos_tag(sentence)\n",
    "    print(word_tags)\n",
    "    for word_tag in word_tags:\n",
    "        if (word_tag[1] == \"NP\" or word_tag[1] == \"VP\"):\n",
    "            acceptable_words.append(word_tag[0])\n",
    "    return acceptable_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 CLUSTERING\n",
    "\n",
    "def cluster_sentences(document, sentences):\n",
    "    X = vectorize(document)\n",
    "    bcl = Birch(branching_factor=10, n_clusters=None, threshold=0.3).fit(X) # the algorithm figures out the clusters\n",
    "    clusters = bcl.predict(X)\n",
    "    labels = bcl.labels_\n",
    "    norm_X = normalize_vector(X, labels)\n",
    "    viz_clusters(norm_X,labels) # visualization before finding the mean\n",
    "    cluster_means = calculate_mean(norm_X)\n",
    "    cluster_sentences = find_minimum_from_mean(cluster_means, norm_X)\n",
    "#     viz_clusters_after(cluster_sentences, set(labels)) # visualization after finding the closest to mean\n",
    "    sents = vectors_to_sentences(cluster_sentences, norm_X, sentences)\n",
    "    sentence_data = draw_table(norm_X,cluster_means,cluster_sentences)\n",
    "    sentence_data = pd.DataFrame(sentence_data, columns = ['Sentence', 'Vector_Value', ' Cluster_ID', 'Cluster_Mean', 'Closest_Sentence_Vector'])\n",
    "    return sents, sentence_data\n",
    "\n",
    "def calculate_mean(X):\n",
    "    clusters = set([sent[0] for sent in X ])\n",
    "    cluster_mean = []\n",
    "    for cluster in clusters:\n",
    "        cluster_value = 0\n",
    "        count = 0\n",
    "        for i,element in enumerate(X):\n",
    "            if (cluster == element[0]):\n",
    "                cluster_value += element[1]\n",
    "                count += 1\n",
    "        mean_cluster_value = cluster_value/count\n",
    "        cluster_mean.append([cluster,mean_cluster_value])\n",
    "    print(\"Cluster Means:\")\n",
    "    print(cluster_mean)\n",
    "    return cluster_mean\n",
    "\n",
    "def find_minimum_from_mean(cluster_means, vectorized):\n",
    "    \"\"\"\n",
    "    This function computes the minimal distance between each element in X and the elements in Y\n",
    "    cluster_means : A 2D array\n",
    "    points : A 2D array consisting of cluster type and the corresponding value\n",
    "\n",
    "    It returns an array which consist of point in X and the corresponding point in Y with the smallest distance the point in X\n",
    "\n",
    "    \"\"\"\n",
    "    minimal_distances = []\n",
    "    for clm in cluster_means:\n",
    "        points_in_cluster = [v[1] for v in vectorized if v[0] == clm[0]]\n",
    "        minimal = points_in_cluster[0]\n",
    "        for pt in points_in_cluster:\n",
    "            diff_current = abs(clm[1] - pt) \n",
    "            diff_minimal = clm[1] - minimal\n",
    "            if (diff_current < diff_minimal):\n",
    "                  minimal = pt\n",
    "        minimal_distances.append((clm[0],minimal))\n",
    "    print(\"Minimal\", minimal_distances)\n",
    "    return minimal_distances\n",
    "\n",
    "def normalize_vector(v, labels):\n",
    "    vectorized = []\n",
    "    for i in range(len(v)):\n",
    "        mean = abs(sum(v[i]/len(v[i])))\n",
    "        vectorized.append((labels[i],mean))\n",
    "    return vectorized\n",
    "\n",
    "def vectors_to_sentences(clustered_sentences, sentence_vector, sentences):\n",
    "    print(\"Sentence Vectors == sentences\")\n",
    "    print(len(sentence_vector), len(sentences)) \n",
    "    index = 0\n",
    "    selected_sentences = []\n",
    "    top_sentences = [s[1] for s in clustered_sentences ]\n",
    "    for v in sentence_vector:\n",
    "        if v[1] in top_sentences:\n",
    "            print(v[1])\n",
    "            selected_sentences.append((index,sentences[index]))\n",
    "        index +=1\n",
    "    return selected_sentences\n",
    "\n",
    "def vectorize(document):\n",
    "    words = word_tokenize(document)\n",
    "    words = preprocess(words)\n",
    "    X = [TaggedDocument(word,[idnx]) for idnx,word in enumerate(words)]\n",
    "    vectorizer = Doc2Vec(X, size=10)\n",
    "    sentence_vectors = [vectorizer.infer_vector(word_tokenize(sen), alpha=2,steps=400) for sen in sent_tokenize(document)]\n",
    "    return sentence_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 RANKING AND GENERATING THE FINAL SUMMARY\n",
    "def generate_summary(sentences,scores,threshold):\n",
    "        ranked_scores = sorted(scores, key=lambda sc:sc[1], reverse=True)\n",
    "        summary = []\n",
    "        for sent_rank in ranked_scores:\n",
    "            summary.append((sent_rank[0], sentences[sent_rank[0]]))\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARIZE DOCUMENT \n",
    "def summarizeDocument(document):\n",
    "    \"\"\"\n",
    "    functions takes a document and returns its summary\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(document)\n",
    "    threshold = set_threshold(sentences) \n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    indexed_sentences, sentence_data = cluster_sentences(document, sentences) \n",
    "    summary = [sentence[1] for sentence in indexed_sentences]\n",
    "    return summary, sentence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Means:\n",
      "[[0, 0.070557181412975], [1, 0.007786415107375433], [2, 0.054084353148937225], [3, 0.15648114681243896]]\n",
      "Minimal [(0, 0.08386647701263428), (1, 0.010006737618823536), (2, 0.05506868101656437), (3, 0.15648114681243896)]\n",
      "Sentence Vectors == sentences\n",
      "14 14\n",
      "0.08386647701263428\n",
      "0.05506868101656437\n",
      "0.010006737618823536\n",
      "0.15648114681243896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Manually creating summaries can be a time\\xadconsuming activity. The proposed system consist of uses text clustering as a precursor to text summarization in order to \\ncreate extractive summaries. The second approach is text clustering which is implemented using the Balanced Iterative and \\nReducing Clustering Using Hierarchies (BIRCH) algorithm. Since documents are usually structured in such a way that different ideas are \\nexpressed at different sections (Rananavare & Reddy, 2017), the proposed system utilizes text \\nclustering as a technique to handle this information spread as well reducing redundancy in the produced\\nsummaries by clustering sentences that most alike together and then picking a representative sentence \\nfrom each cluster.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = open('./raw_text/test__3.txt').read()\n",
    "summary,sentence_data = summarizeDocument(document)\n",
    "\" \".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('./raw_text/summaries/summary3.txt', 'w+')\n",
    "file.write(\"\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
